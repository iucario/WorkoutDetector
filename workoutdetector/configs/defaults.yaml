trainer:
  default_root_dir: exp/default # where to save logs and checkpoints
  max_epochs: 50
  enable_checkpointing: true
  num_nodes: 1
  devices: auto
  gpus: auto
  log_gpu_memory: null
  enable_progress_bar: true
  overfit_batches: 0.0
  check_val_every_n_epoch: 1
  min_epochs: 1
  accelerator: auto
  sync_batchnorm: false
  precision: 32
  enable_model_summary: true
  weights_summary: top
  num_sanity_val_steps: 2
  resume_from_checkpoint: null
  benchmark: false
  deterministic: true
  auto_lr_find: false
  auto_scale_batch_size: null
  prepare_data_per_node: null
  fast_dev_run: false

optimizer:
  method: TSN # `TSN` or `TDN` when using TSN based models
  lr: 0.005
  momentum: 0.9
  weight_decay: 5.0e-4
  eps: 1.0e-8

lr_scheduler:
  policy: StepLR
  gamma: 0.1 # MultiStepLR lr decay rate
  step: 8 # StepLR step_size
  warmup_epoch: 0
  warmup_multiplier: 100.0
  steps: [10, 20] # MultiStepLR milestones

model:
  model_type: TSM
  num_class: 12
  num_segments: 8

  # Frames around the center to calculate difference. Used in Temportal Difference Network
  num_frames: 1
  example_input_array: [1, 8, 3, 224, 224] # Will be reshaped to [-1, 3, 224, 224] anyway
  base_model: resnet50
  consensus_type: avg
  dropout: 0.5
  partial_bn: false
  img_feature_dim: 256
  is_shift: true
  shift_div: 8
  shift_place: blockres
  fc_lr5: false
  temporal_pool: false
  non_local: false
  checkpoint: checkpoints/finetune/TSM_somethingv2_RGB_resnet50_shift8_blockres_avg_segment8_e45.pth

data:
  dataset_type: FrameDataset
  data_root: /home/user/data
  num_segments: 8
  num_frames: 1 # used to calculate difference
  filename_tmpl: "img_{:05}.jpg"
  anno_col: 4
  batch_size: 4
  train:
    anno: /home/user/data/Binary/all-train.txt
    data_prefix: null
    transform:
      person_crop: false
  val:
    anno: /home/user/data/Binary/all-val.txt
    data_prefix: null
    transform:
      person_crop: false
  test:
    anno: /home/user/data/Binary/all-test.txt
    data_prefix: null
    transform:
      person_crop: false
  num_workers: 8

log:
  output_dir: null # os.path.join(trainer.default_root_dir, timestamp)
  log_every_n_steps: 20
  csv:
    enable: true
  tensorboard:
    enable: true
  wandb:
    enable: true
    offline: false
    project: default
    name: null

callbacks:
  modelcheckpoint:
    save_top_k: 1
    save_weights_only: false
    monitor: val/acc
    mode: max
    dirpath: null # if None, defaults to log.output_dir
  early_stopping:
    enable: false
    patience: 10

seed: 0
train: true
timestamp: null # Will be initialized in python file. Bug.
# Creates duplicate folders because of Pytorch Lightning's DDP strategy.
# Better input from shell script `ts=$(date +%Y%m%d-%H%M%S)`
